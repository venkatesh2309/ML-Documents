{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": true
    },
    "colab": {
      "name": "3.13 Hybrid_Recursive_feature_elimination.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fid1E7YhWxK",
        "colab_type": "text"
      },
      "source": [
        "## Hybrid method: Recursive feature elimination\n",
        "\n",
        "This method consists of the following steps:\n",
        "\n",
        "1) Rank the features according to their importance derived from a machine learning algorithm: it can be tree importance, or LASSO / Ridge, or the linear / logistic regression coefficients.\n",
        "\n",
        "2) Remove one feature -the least important- and build a machine learning algorithm utilising the remaining features.\n",
        "\n",
        "3) Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
        "\n",
        "4) If the metric decreases by more of an arbitrarily set threshold, then that feature is important and should be kept. Otherwise, we can remove that feature.\n",
        "\n",
        "5) Repeat steps 2-4 until all features have been removed (and therefore evaluated) and the drop in performance assessed.\n",
        "\n",
        "\n",
        "I call this a hybrid method because:\n",
        "\n",
        "- it combines the importance derived from the machine learning algorithm like embedded methods,\n",
        "- and it removes as well one feature at a time, and calculates a new metric based on the new subset of features and the machine learning algorithm of choice, like wrapper methods.\n",
        "\n",
        "The difference between this method and the step backwards feature selection we learned in previous lectures lies in that it does not remove all features first in order to determine which one to remove. It removes the least important one, based on the machine learning model derived important. And then, it makes an assessment as to whether that feature should be removed or not. So it removes each feature only once during selection, whereas step backward feature selection removes all the features at each step of selection.\n",
        "\n",
        "This method is therefore faster than wrapper methods and generally better than embedded methods. In practice it works extremely well. It does also account for correlations (depending on how stringent you set the arbitrary performance drop threshold). On the downside, the drop in performance assessed to decide whether the feature should be kept or removed, is set arbitrarily. The smaller the drop the more features will be selected, and vice versa.\n",
        "\n",
        "I will demonstrate how to select features using this method on a regression and classification problem. For classification I will use the Paribas claims dataset from Kaggle. For regression, the House Price dataset from Kaggle.\n",
        "\n",
        "**Note** For the demonstration, I will use XGBoost, but this method is useful for any machine learning algorithm. In fact, the importance of the features are determined specifically for the algorithm used. Therefore, different algorithms may return different subsets of important features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTS67rclMack",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "0d72d7d8-ce10-4225-b91c-e9a1ba289b2b"
      },
      "source": [
        "!pip install --user kaggle\n",
        "!mkdir .kaggle\n",
        "import json\n",
        "token = {\"username\":\"###\",\"key\":\"####\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "mkdir: cannot create directory ‘.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP8VJociMtAa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "fa3edb52-1edb-40b5-a304-52f0eb9d47c5"
      },
      "source": [
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c bnp-paribas-cardif-claims-management\n",
        "!unzip train.csv.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv.zip to /content\n",
            " 55% 27.0M/49.4M [00:00<00:00, 125MB/s]\n",
            "100% 49.4M/49.4M [00:00<00:00, 165MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/162k [00:00<?, ?B/s]\n",
            "100% 162k/162k [00:00<00:00, 151MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 81% 40.0M/49.4M [00:00<00:00, 166MB/s]\n",
            "100% 49.4M/49.4M [00:00<00:00, 196MB/s]\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbb_LYSahWxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxOWskK5hWxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "523cd16b-a483-4ebe-a26e-513a24b1b3cb"
      },
      "source": [
        "# load dataset\n",
        "data = pd.read_csv('train.csv', nrows=50000)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 133)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24TezfDXhWxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "454c0b30-90b5-4f8a-8b21-52cc4c7c333a"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>target</th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>v3</th>\n",
              "      <th>v4</th>\n",
              "      <th>v5</th>\n",
              "      <th>v6</th>\n",
              "      <th>v7</th>\n",
              "      <th>v8</th>\n",
              "      <th>v9</th>\n",
              "      <th>v10</th>\n",
              "      <th>v11</th>\n",
              "      <th>v12</th>\n",
              "      <th>v13</th>\n",
              "      <th>v14</th>\n",
              "      <th>v15</th>\n",
              "      <th>v16</th>\n",
              "      <th>v17</th>\n",
              "      <th>v18</th>\n",
              "      <th>v19</th>\n",
              "      <th>v20</th>\n",
              "      <th>v21</th>\n",
              "      <th>v22</th>\n",
              "      <th>v23</th>\n",
              "      <th>v24</th>\n",
              "      <th>v25</th>\n",
              "      <th>v26</th>\n",
              "      <th>v27</th>\n",
              "      <th>v28</th>\n",
              "      <th>v29</th>\n",
              "      <th>v30</th>\n",
              "      <th>v31</th>\n",
              "      <th>v32</th>\n",
              "      <th>v33</th>\n",
              "      <th>v34</th>\n",
              "      <th>v35</th>\n",
              "      <th>v36</th>\n",
              "      <th>v37</th>\n",
              "      <th>v38</th>\n",
              "      <th>...</th>\n",
              "      <th>v92</th>\n",
              "      <th>v93</th>\n",
              "      <th>v94</th>\n",
              "      <th>v95</th>\n",
              "      <th>v96</th>\n",
              "      <th>v97</th>\n",
              "      <th>v98</th>\n",
              "      <th>v99</th>\n",
              "      <th>v100</th>\n",
              "      <th>v101</th>\n",
              "      <th>v102</th>\n",
              "      <th>v103</th>\n",
              "      <th>v104</th>\n",
              "      <th>v105</th>\n",
              "      <th>v106</th>\n",
              "      <th>v107</th>\n",
              "      <th>v108</th>\n",
              "      <th>v109</th>\n",
              "      <th>v110</th>\n",
              "      <th>v111</th>\n",
              "      <th>v112</th>\n",
              "      <th>v113</th>\n",
              "      <th>v114</th>\n",
              "      <th>v115</th>\n",
              "      <th>v116</th>\n",
              "      <th>v117</th>\n",
              "      <th>v118</th>\n",
              "      <th>v119</th>\n",
              "      <th>v120</th>\n",
              "      <th>v121</th>\n",
              "      <th>v122</th>\n",
              "      <th>v123</th>\n",
              "      <th>v124</th>\n",
              "      <th>v125</th>\n",
              "      <th>v126</th>\n",
              "      <th>v127</th>\n",
              "      <th>v128</th>\n",
              "      <th>v129</th>\n",
              "      <th>v130</th>\n",
              "      <th>v131</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.335739</td>\n",
              "      <td>8.727474</td>\n",
              "      <td>C</td>\n",
              "      <td>3.921026</td>\n",
              "      <td>7.915266</td>\n",
              "      <td>2.599278</td>\n",
              "      <td>3.176895</td>\n",
              "      <td>0.012941</td>\n",
              "      <td>9.999999</td>\n",
              "      <td>0.503281</td>\n",
              "      <td>16.434108</td>\n",
              "      <td>6.085711</td>\n",
              "      <td>2.866830</td>\n",
              "      <td>11.636387</td>\n",
              "      <td>1.355013</td>\n",
              "      <td>8.571429</td>\n",
              "      <td>3.670350</td>\n",
              "      <td>0.106720</td>\n",
              "      <td>0.148883</td>\n",
              "      <td>18.869283</td>\n",
              "      <td>7.730923</td>\n",
              "      <td>XDX</td>\n",
              "      <td>-1.716131e-08</td>\n",
              "      <td>C</td>\n",
              "      <td>0.139412</td>\n",
              "      <td>1.720818</td>\n",
              "      <td>3.393503</td>\n",
              "      <td>0.590122</td>\n",
              "      <td>8.880867</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>1.083033</td>\n",
              "      <td>1.010829</td>\n",
              "      <td>7.270147</td>\n",
              "      <td>8.375452</td>\n",
              "      <td>11.326592</td>\n",
              "      <td>0.454546</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.442252</td>\n",
              "      <td>5.814018</td>\n",
              "      <td>3.517720</td>\n",
              "      <td>0.462019</td>\n",
              "      <td>7.436824</td>\n",
              "      <td>5.454545</td>\n",
              "      <td>8.877414</td>\n",
              "      <td>1.191337</td>\n",
              "      <td>19.470199</td>\n",
              "      <td>8.389237</td>\n",
              "      <td>2.757375</td>\n",
              "      <td>4.374296</td>\n",
              "      <td>1.574039</td>\n",
              "      <td>0.007294</td>\n",
              "      <td>12.579184</td>\n",
              "      <td>E</td>\n",
              "      <td>2.382692</td>\n",
              "      <td>3.930922</td>\n",
              "      <td>B</td>\n",
              "      <td>0.433213</td>\n",
              "      <td>O</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.634907</td>\n",
              "      <td>2.857144</td>\n",
              "      <td>1.951220</td>\n",
              "      <td>6.592012</td>\n",
              "      <td>5.909091</td>\n",
              "      <td>-6.297423e-07</td>\n",
              "      <td>1.059603</td>\n",
              "      <td>0.803572</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1.989780</td>\n",
              "      <td>0.035754</td>\n",
              "      <td>AU</td>\n",
              "      <td>1.804126</td>\n",
              "      <td>3.113719</td>\n",
              "      <td>2.024285</td>\n",
              "      <td>0</td>\n",
              "      <td>0.636365</td>\n",
              "      <td>2.857144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.191265</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.301630</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.312910</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.507647</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.636386</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.763110</td>\n",
              "      <td>GUV</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>3.056144</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.615077</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.579479</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.303967</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.505335</td>\n",
              "      <td>NaN</td>\n",
              "      <td>B</td>\n",
              "      <td>1.825361</td>\n",
              "      <td>4.247858</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>U</td>\n",
              "      <td>G</td>\n",
              "      <td>10.308044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.595357</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.598896</td>\n",
              "      <td>AF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.957825</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.943877</td>\n",
              "      <td>5.310079</td>\n",
              "      <td>C</td>\n",
              "      <td>4.410969</td>\n",
              "      <td>5.326159</td>\n",
              "      <td>3.979592</td>\n",
              "      <td>3.928571</td>\n",
              "      <td>0.019645</td>\n",
              "      <td>12.666667</td>\n",
              "      <td>0.765864</td>\n",
              "      <td>14.756098</td>\n",
              "      <td>6.384670</td>\n",
              "      <td>2.505589</td>\n",
              "      <td>9.603542</td>\n",
              "      <td>1.984127</td>\n",
              "      <td>5.882353</td>\n",
              "      <td>3.170847</td>\n",
              "      <td>0.244541</td>\n",
              "      <td>0.144258</td>\n",
              "      <td>17.952332</td>\n",
              "      <td>5.245035</td>\n",
              "      <td>FQ</td>\n",
              "      <td>-2.785053e-07</td>\n",
              "      <td>E</td>\n",
              "      <td>0.113997</td>\n",
              "      <td>2.244897</td>\n",
              "      <td>5.306122</td>\n",
              "      <td>0.836005</td>\n",
              "      <td>7.499999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>1.454082</td>\n",
              "      <td>1.734693</td>\n",
              "      <td>4.043864</td>\n",
              "      <td>7.959184</td>\n",
              "      <td>12.730517</td>\n",
              "      <td>0.259740</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.271480</td>\n",
              "      <td>5.156559</td>\n",
              "      <td>4.214944</td>\n",
              "      <td>0.309657</td>\n",
              "      <td>5.663265</td>\n",
              "      <td>5.974026</td>\n",
              "      <td>11.588858</td>\n",
              "      <td>0.841837</td>\n",
              "      <td>15.491329</td>\n",
              "      <td>5.879353</td>\n",
              "      <td>3.292788</td>\n",
              "      <td>5.924457</td>\n",
              "      <td>1.668401</td>\n",
              "      <td>0.008275</td>\n",
              "      <td>11.670572</td>\n",
              "      <td>C</td>\n",
              "      <td>1.375753</td>\n",
              "      <td>1.184211</td>\n",
              "      <td>B</td>\n",
              "      <td>3.367348</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.205561</td>\n",
              "      <td>12.941177</td>\n",
              "      <td>3.129253</td>\n",
              "      <td>3.478911</td>\n",
              "      <td>6.233767</td>\n",
              "      <td>-2.792745e-07</td>\n",
              "      <td>2.138728</td>\n",
              "      <td>2.238806</td>\n",
              "      <td>9.333333</td>\n",
              "      <td>2.477596</td>\n",
              "      <td>0.013452</td>\n",
              "      <td>AE</td>\n",
              "      <td>1.773709</td>\n",
              "      <td>3.922193</td>\n",
              "      <td>1.120468</td>\n",
              "      <td>2</td>\n",
              "      <td>0.883118</td>\n",
              "      <td>1.176472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.797415</td>\n",
              "      <td>8.304757</td>\n",
              "      <td>C</td>\n",
              "      <td>4.225930</td>\n",
              "      <td>11.627438</td>\n",
              "      <td>2.097700</td>\n",
              "      <td>1.987549</td>\n",
              "      <td>0.171947</td>\n",
              "      <td>8.965516</td>\n",
              "      <td>6.542669</td>\n",
              "      <td>16.347483</td>\n",
              "      <td>9.646653</td>\n",
              "      <td>3.903302</td>\n",
              "      <td>14.094723</td>\n",
              "      <td>1.945044</td>\n",
              "      <td>5.517242</td>\n",
              "      <td>3.610789</td>\n",
              "      <td>1.224114</td>\n",
              "      <td>0.231630</td>\n",
              "      <td>18.376407</td>\n",
              "      <td>7.517125</td>\n",
              "      <td>ACUE</td>\n",
              "      <td>-4.805344e-07</td>\n",
              "      <td>D</td>\n",
              "      <td>0.148843</td>\n",
              "      <td>1.308269</td>\n",
              "      <td>2.303640</td>\n",
              "      <td>8.926662</td>\n",
              "      <td>8.874521</td>\n",
              "      <td>C</td>\n",
              "      <td>B</td>\n",
              "      <td>1.587644</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>8.703550</td>\n",
              "      <td>8.898468</td>\n",
              "      <td>11.302795</td>\n",
              "      <td>0.433735</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.763925</td>\n",
              "      <td>5.498902</td>\n",
              "      <td>3.423944</td>\n",
              "      <td>0.832518</td>\n",
              "      <td>7.375480</td>\n",
              "      <td>6.746988</td>\n",
              "      <td>6.942002</td>\n",
              "      <td>1.334611</td>\n",
              "      <td>18.256352</td>\n",
              "      <td>8.507281</td>\n",
              "      <td>2.503055</td>\n",
              "      <td>4.872157</td>\n",
              "      <td>2.573664</td>\n",
              "      <td>0.113967</td>\n",
              "      <td>12.554274</td>\n",
              "      <td>B</td>\n",
              "      <td>2.230754</td>\n",
              "      <td>1.990131</td>\n",
              "      <td>B</td>\n",
              "      <td>2.643678</td>\n",
              "      <td>J</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.777666</td>\n",
              "      <td>10.574713</td>\n",
              "      <td>1.511063</td>\n",
              "      <td>4.949609</td>\n",
              "      <td>7.180722</td>\n",
              "      <td>5.655086e-01</td>\n",
              "      <td>1.166281</td>\n",
              "      <td>1.956521</td>\n",
              "      <td>7.018256</td>\n",
              "      <td>1.812795</td>\n",
              "      <td>0.002267</td>\n",
              "      <td>CJ</td>\n",
              "      <td>1.415230</td>\n",
              "      <td>2.954381</td>\n",
              "      <td>1.990847</td>\n",
              "      <td>1</td>\n",
              "      <td>1.677108</td>\n",
              "      <td>1.034483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.050328</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.320087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.991098</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.414567</td>\n",
              "      <td>HIT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>E</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.083151</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>14.097099</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 133 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  target        v1        v2  ...      v128  v129      v130      v131\n",
              "0   3       1  1.335739  8.727474  ...  2.024285     0  0.636365  2.857144\n",
              "1   4       1       NaN       NaN  ...  1.957825     0       NaN       NaN\n",
              "2   5       1  0.943877  5.310079  ...  1.120468     2  0.883118  1.176472\n",
              "3   6       1  0.797415  8.304757  ...  1.990847     1  1.677108  1.034483\n",
              "4   8       1       NaN       NaN  ...       NaN     0       NaN       NaN\n",
              "\n",
              "[5 rows x 133 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DREo38wVhWxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a20912e4-53bf-4512-bbda-a8bf6ff2bc3d"
      },
      "source": [
        "# In practice, feature selection should be done after data pre-processing,\n",
        "# so ideally, all the categorical variables are encoded into numbers,\n",
        "# and then you can assess how deterministic they are of the target\n",
        "\n",
        "# here for simplicity I will use only numerical variables\n",
        "# select numerical columns:\n",
        "\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
        "data = data[numerical_vars]\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 114)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNBCvUrohWxj",
        "colab_type": "text"
      },
      "source": [
        "### Important\n",
        "\n",
        "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vihIQo9hWxk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebcf8c9e-dc14-4295-bfc3-1f92700295ef"
      },
      "source": [
        "# separate train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop(labels=['target', 'ID'], axis=1),\n",
        "    data['target'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((35000, 112), (15000, 112))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "oIfez4_ehWxq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9527512a-394e-4fc4-a0c2-a4c10457618d"
      },
      "source": [
        "# the first step of this procedure  consists in building\n",
        "# a machine learning algorithm using all the available features\n",
        "# and then determine the importance of the features according\n",
        "# to the algorithm\n",
        "\n",
        "# set the seed for reproducibility\n",
        "seed_val = 1000000000\n",
        "np.random.seed(seed_val)\n",
        "\n",
        "# build initial model using all the features\n",
        "model_all_features = xgb.XGBClassifier(\n",
        "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
        "\n",
        "model_all_features.fit(X_train, y_train)\n",
        "\n",
        "# calculate the roc-auc in the test set\n",
        "y_pred_test = model_all_features.predict_proba(X_test)[:, 1]\n",
        "auc_score_all = roc_auc_score(y_test, y_pred_test)\n",
        "print('Test all features xgb ROC AUC=%f' % (auc_score_all))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test all features xgb ROC AUC=0.713140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeemzP8phWxy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "e53896af-a8f9-423f-c9ef-79e59dcd65eb"
      },
      "source": [
        "# the second step consist of deriving the importance of \n",
        "# each feature and ranking them from the least to the most\n",
        "# important\n",
        "\n",
        "# get feature name and importance\n",
        "features = pd.Series(model_all_features.feature_importances_)\n",
        "features.index = X_train.columns\n",
        "\n",
        "# sort the features by importance\n",
        "features.sort_values(ascending=True, inplace=True)\n",
        "\n",
        "# plot\n",
        "features.plot.bar(figsize=(20,6))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7faec390acc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIcAAAF0CAYAAAC5T1mLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmT8m5kMhWx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af0cf4a9-2091-47c6-f6bf-690df778c193"
      },
      "source": [
        "# view the list of ordered features\n",
        "features = list(features.index)\n",
        "features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['v46',\n",
              " 'v61',\n",
              " 'v49',\n",
              " 'v96',\n",
              " 'v48',\n",
              " 'v4',\n",
              " 'v83',\n",
              " 'v2',\n",
              " 'v44',\n",
              " 'v108',\n",
              " 'v1',\n",
              " 'v80',\n",
              " 'v54',\n",
              " 'v118',\n",
              " 'v124',\n",
              " 'v130',\n",
              " 'v127',\n",
              " 'v55',\n",
              " 'v119',\n",
              " 'v28',\n",
              " 'v18',\n",
              " 'v59',\n",
              " 'v88',\n",
              " 'v69',\n",
              " 'v111',\n",
              " 'v121',\n",
              " 'v82',\n",
              " 'v42',\n",
              " 'v102',\n",
              " 'v5',\n",
              " 'v76',\n",
              " 'v27',\n",
              " 'v77',\n",
              " 'v16',\n",
              " 'v98',\n",
              " 'v19',\n",
              " 'v17',\n",
              " 'v97',\n",
              " 'v68',\n",
              " 'v41',\n",
              " 'v94',\n",
              " 'v15',\n",
              " 'v57',\n",
              " 'v78',\n",
              " 'v26',\n",
              " 'v70',\n",
              " 'v100',\n",
              " 'v120',\n",
              " 'v90',\n",
              " 'v104',\n",
              " 'v81',\n",
              " 'v85',\n",
              " 'v86',\n",
              " 'v25',\n",
              " 'v39',\n",
              " 'v103',\n",
              " 'v23',\n",
              " 'v58',\n",
              " 'v9',\n",
              " 'v45',\n",
              " 'v101',\n",
              " 'v29',\n",
              " 'v84',\n",
              " 'v122',\n",
              " 'v20',\n",
              " 'v131',\n",
              " 'v65',\n",
              " 'v43',\n",
              " 'v51',\n",
              " 'v99',\n",
              " 'v87',\n",
              " 'v21',\n",
              " 'v53',\n",
              " 'v11',\n",
              " 'v13',\n",
              " 'v33',\n",
              " 'v36',\n",
              " 'v32',\n",
              " 'v73',\n",
              " 'v7',\n",
              " 'v95',\n",
              " 'v126',\n",
              " 'v8',\n",
              " 'v117',\n",
              " 'v109',\n",
              " 'v63',\n",
              " 'v6',\n",
              " 'v35',\n",
              " 'v89',\n",
              " 'v67',\n",
              " 'v116',\n",
              " 'v12',\n",
              " 'v92',\n",
              " 'v64',\n",
              " 'v106',\n",
              " 'v93',\n",
              " 'v40',\n",
              " 'v37',\n",
              " 'v128',\n",
              " 'v60',\n",
              " 'v123',\n",
              " 'v115',\n",
              " 'v105',\n",
              " 'v72',\n",
              " 'v14',\n",
              " 'v34',\n",
              " 'v10',\n",
              " 'v114',\n",
              " 'v38',\n",
              " 'v62',\n",
              " 'v129',\n",
              " 'v50']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szLGUjmVhWx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the final step consists in removing one at a time\n",
        "# all the features, from the least to the most\n",
        "# important, and build an xgboost at each round.\n",
        "\n",
        "# once we build the model, we calculate the new roc-auc\n",
        "# if the new roc-auc is smaller than the original one\n",
        "# (with all the features), then that feature that was removed\n",
        "# was important, and we should keep it.\n",
        "# otherwise, we should remove the feature\n",
        "\n",
        "# recursive feature elimination:\n",
        "\n",
        "# first we arbitrarily set the drop in roc-auc\n",
        "# if the drop is below this threshold,\n",
        "# the feature will be removed\n",
        "tol = 0.0005\n",
        "\n",
        "print('doing recursive feature elimination')\n",
        "\n",
        "# we initialise a list where we will collect the\n",
        "# features we should remove\n",
        "features_to_remove = []\n",
        "\n",
        "# set a counter to know how far ahead the loop is going\n",
        "count = 1\n",
        "\n",
        "# now we loop over all the features, in order of importance:\n",
        "# remember that features is the list of ordered features\n",
        "# by importance\n",
        "for feature in features:\n",
        "    print()\n",
        "    print('testing feature: ', feature, ' which is feature ', count,\n",
        "          ' out of ', len(features))\n",
        "    count = count + 1\n",
        "\n",
        "    # initialise model\n",
        "    model_int = xgb.XGBClassifier(\n",
        "        nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
        "\n",
        "    # fit model with all variables minus the removed features\n",
        "    # and the feature to be evaluated\n",
        "    model_int.fit(\n",
        "        X_train.drop(features_to_remove + [feature], axis=1), y_train)\n",
        "\n",
        "    # make a prediction over the test set\n",
        "    y_pred_test = model_int.predict_proba(\n",
        "        X_test.drop(features_to_remove + [feature], axis=1))[:, 1]\n",
        "\n",
        "    # calculate the new roc-auc\n",
        "    auc_score_int = roc_auc_score(y_test, y_pred_test)\n",
        "    print('New Test ROC AUC={}'.format((auc_score_int)))\n",
        "\n",
        "    # print the original roc-auc with all the features\n",
        "    print('All features Test ROC AUC={}'.format((auc_score_all)))\n",
        "\n",
        "    # determine the drop in the roc-auc\n",
        "    diff_auc = auc_score_all - auc_score_int\n",
        "\n",
        "    # compare the drop in roc-auc with the tolerance\n",
        "    # we set previously\n",
        "    if diff_auc >= tol:\n",
        "        print('Drop in ROC AUC={}'.format(diff_auc))\n",
        "        print('keep: ', feature)\n",
        "        print\n",
        "    else:\n",
        "        print('Drop in ROC AUC={}'.format(diff_auc))\n",
        "        print('remove: ', feature)\n",
        "        print\n",
        "        # if the drop in the roc is small and we remove the\n",
        "        # feature, we need to set the new roc to the one based on\n",
        "        # the remaining features\n",
        "        auc_score_all = auc_score_int\n",
        "        \n",
        "        # and append the feature to remove to the collecting\n",
        "        # list\n",
        "        features_to_remove.append(feature)\n",
        "\n",
        "# now the loop is finished, we evaluated all the features\n",
        "print('DONE!!')\n",
        "print('total features to remove: ', len(features_to_remove))\n",
        "\n",
        "# determine the features to keep (those we won't remove)\n",
        "features_to_keep = [x for x in features if x not in features_to_remove]\n",
        "print('total features to keep: ', len(features_to_keep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWaIqvvvhWyE",
        "colab_type": "code",
        "colab": {},
        "outputId": "428b26c7-15f4-4640-daf2-2be7ced2f40a"
      },
      "source": [
        "# capture the 56 selected features\n",
        "seed_val = 1000000000\n",
        "np.random.seed(seed_val)\n",
        "\n",
        "# build initial model\n",
        "final_xgb = xgb.XGBClassifier(\n",
        "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
        "\n",
        "# fit the model with the selected features\n",
        "final_xgb.fit(X_train[features_to_keep], y_train)\n",
        "\n",
        "# make predictions\n",
        "y_pred_test = final_xgbl.predict_proba(X_test[features_to_keep])[:, 1]\n",
        "\n",
        "# calculate roc-auc\n",
        "auc_score_final = roc_auc_score(y_test, y_pred_test)\n",
        "print('Test selected features ROC AUC=%f' % (auc_score_final))\n",
        "print('Test all features ROC AUC=%f' % (auc_score_all))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test selected features ROC AUC=0.715491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQIosU2ohWyJ",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the xgb model built with 56 features shows a similar performance than the one built with the total features (0.715 vs 0.713).\n",
        "\n",
        "We may not be able to get this right from the beginning though, as we did here. This method of feature selection does require that you try a few different tolerances / thresholds until you find the right number of features.\n",
        "\n",
        "Why don't you go ahead and try different values? Try with lower and bigger thresholds and get a feeling of how much this affects the number of selected features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM-oagQChWyL",
        "colab_type": "text"
      },
      "source": [
        "### Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI_2F6SVhWyN",
        "colab_type": "code",
        "colab": {},
        "outputId": "b7d4c475-ab5e-4a70-ee2f-0dcb3c856836"
      },
      "source": [
        "# load dataset\n",
        "data = pd.read_csv('houseprice.csv', nrows=50000)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 81)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CYZCy0-hWyS",
        "colab_type": "code",
        "colab": {},
        "outputId": "00558b2c-9c95-4d9e-beb5-e5f540717c1f"
      },
      "source": [
        "# In practice, feature selection should be done after data pre-processing,\n",
        "# so ideally, all the categorical variables are encoded into numbers,\n",
        "# and then you can assess how deterministic they are of the target\n",
        "\n",
        "# here for simplicity I will use only numerical variables\n",
        "# select numerical columns:\n",
        "\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
        "data = data[numerical_vars]\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 38)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epywE9EFhWyW",
        "colab_type": "code",
        "colab": {},
        "outputId": "26bded83-24d7-40f3-8dac-5dc311b701b0"
      },
      "source": [
        "# separate train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop(labels=['Id','SalePrice'], axis=1),\n",
        "    data['SalePrice'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1022, 36), (438, 36))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "duHwkswMhWyb",
        "colab_type": "code",
        "colab": {},
        "outputId": "f756c0f5-9799-49a2-c0c1-7da6b6e4c378"
      },
      "source": [
        "# the first step of this procedure consists in building\n",
        "# a machine learning algorithm using all the available features\n",
        "# and then determine the importance of the features according\n",
        "# to the algorithm\n",
        "\n",
        "# set the seed for reproducibility\n",
        "seed_val = 1000000000\n",
        "np.random.seed(seed_val)\n",
        "\n",
        "# build initial model using all the features\n",
        "model_all_features = xgb.XGBRegressor(\n",
        "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
        "\n",
        "model_all_features.fit(X_train, y_train)\n",
        "\n",
        "# calculate the roc-auc in the test set\n",
        "y_pred_test = model_all_features.predict(X_test)\n",
        "r2_score_all = r2_score(y_test, y_pred_test)\n",
        "print('Test all features xgb R2 = %f' % (r2_score_all))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test all features xgb R2 = 0.818551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ElkUunKhWyh",
        "colab_type": "code",
        "colab": {},
        "outputId": "3d0206a8-8314-4378-c7e6-7eb34f2cb175"
      },
      "source": [
        "# the second step consist of deriving the importance of \n",
        "# each feature and ranking them from the least to the most\n",
        "# important\n",
        "\n",
        "# get feature name and importance\n",
        "features = pd.Series(model_all_features.feature_importances_)\n",
        "features.index = X_train.columns\n",
        "\n",
        "# sort the features by importance\n",
        "features.sort_values(ascending=True, inplace=True)\n",
        "\n",
        "# plot\n",
        "features.plot.bar(figsize=(20,6))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0xb71ed5cc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIUAAAGiCAYAAAB53XaEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0xb71eace7f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xR3HQLPohWyl",
        "colab_type": "code",
        "colab": {},
        "outputId": "f044bafe-6226-4090-9b04-ca54f6931a82"
      },
      "source": [
        "# view the list of ordered features\n",
        "features = list(features.index)\n",
        "features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LowQualFinSF',\n",
              " 'KitchenAbvGr',\n",
              " 'BsmtFullBath',\n",
              " 'BsmtHalfBath',\n",
              " 'GarageCars',\n",
              " 'MiscVal',\n",
              " 'FullBath',\n",
              " 'HalfBath',\n",
              " 'Fireplaces',\n",
              " '3SsnPorch',\n",
              " 'EnclosedPorch',\n",
              " 'TotRmsAbvGrd',\n",
              " 'BedroomAbvGr',\n",
              " 'PoolArea',\n",
              " 'ScreenPorch',\n",
              " 'YrSold',\n",
              " 'BsmtFinSF2',\n",
              " '2ndFlrSF',\n",
              " 'YearRemodAdd',\n",
              " 'WoodDeckSF',\n",
              " 'OpenPorchSF',\n",
              " 'OverallQual',\n",
              " 'TotalBsmtSF',\n",
              " 'MasVnrArea',\n",
              " 'OverallCond',\n",
              " 'MSSubClass',\n",
              " 'MoSold',\n",
              " 'GarageYrBlt',\n",
              " 'YearBuilt',\n",
              " '1stFlrSF',\n",
              " 'BsmtFinSF1',\n",
              " 'GarageArea',\n",
              " 'GrLivArea',\n",
              " 'BsmtUnfSF',\n",
              " 'LotArea',\n",
              " 'LotFrontage']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hgVslPcvhWyq",
        "colab_type": "code",
        "colab": {},
        "outputId": "fce2d63c-e255-4b2a-e91b-713e82be20a5"
      },
      "source": [
        "# the final step consists in removing one at a time\n",
        "# all the features, from the least to the most\n",
        "# important, and build an xgboost at each round.\n",
        "\n",
        "# once we build the model, we calculate the new r2\n",
        "# if the new r2 is smaller than the original one\n",
        "# (with all the features), then that feature that was removed\n",
        "# was important, and we should keep it.\n",
        "# otherwise, we should remove the feature\n",
        "\n",
        "# recursive feature elimination:\n",
        "\n",
        "# first we arbitrarily set the drop in r2\n",
        "# if the drop is below this threshold,\n",
        "# the feature will be removed\n",
        "tol = 0.001\n",
        "\n",
        "print('doing recursive feature elimination')\n",
        "\n",
        "# we initialise a list where we will collect the\n",
        "# features we should remove\n",
        "features_to_remove = []\n",
        "\n",
        "# set a counter to know how far ahead the loop is going\n",
        "count = 1\n",
        "\n",
        "# now we loop over all the features, in order of importance:\n",
        "# remember that features is the list of ordered features\n",
        "# by importance\n",
        "for feature in features:\n",
        "    print()\n",
        "    print('testing feature: ', feature, ' which is feature ', count,\n",
        "          ' out of ', len(features))\n",
        "    count = count + 1\n",
        "\n",
        "    # initialise model\n",
        "    model_int = xgb.XGBRegressor(\n",
        "        nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
        "\n",
        "    # fit model with all variables minus the removed features\n",
        "    # and the feature to be evaluated\n",
        "    model_int.fit(\n",
        "        X_train.drop(features_to_remove + [feature], axis=1), y_train)\n",
        "\n",
        "    # make a prediction over the test set\n",
        "    y_pred_test = model_int.predict(\n",
        "        X_test.drop(features_to_remove + [feature], axis=1))\n",
        "\n",
        "    # calculate the new r2\n",
        "    r2_score_int = r2_score(y_test, y_pred_test)\n",
        "    print('New Test r2 = {}'.format((r2_score_int)))\n",
        "\n",
        "    # print the original r2 with all the features\n",
        "    print('All features Test r2 = {}'.format((r2_score_all)))\n",
        "\n",
        "    # determine the drop in the r2\n",
        "    diff_r2 = r2_score_all - r2_score_int\n",
        "\n",
        "    # compare the drop in r2 with the tolerance\n",
        "    # we set previously\n",
        "    if diff_r2 >= tol:\n",
        "        print('Drop in r2 ={}'.format(diff_r2))\n",
        "        print('keep: ', feature)\n",
        "        print\n",
        "    else:\n",
        "        print('Drop in r2 = {}'.format(diff_r2))\n",
        "        print('remove: ', feature)\n",
        "        print\n",
        "        # if the drop in the r2 is small and we remove the\n",
        "        # feature, we need to set the new r2 to the one based on\n",
        "        # the remaining features\n",
        "        r2_score_all = r2_score_int\n",
        "        \n",
        "        # and append the feature to remove to the collecting\n",
        "        # list\n",
        "        features_to_remove.append(feature)\n",
        "\n",
        "# now the loop is finished, we evaluated all the features\n",
        "print('DONE!!')\n",
        "print('total features to remove: ', len(features_to_remove))\n",
        "\n",
        "# determine the features to keep (those we won't remove)\n",
        "features_to_keep = [x for x in features if x not in features_to_remove]\n",
        "print('total features to keep: ', len(features_to_keep))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doing recursive feature elimination\n",
            "\n",
            "testing feature:  LowQualFinSF  which is feature  1  out of  36\n",
            "New Test r2 = 0.8176628877400767\n",
            "All features Test r2 = 0.8185508615101988\n",
            "Drop in r2 = 0.0008879737701221746\n",
            "remove:  LowQualFinSF\n",
            "\n",
            "testing feature:  KitchenAbvGr  which is feature  2  out of  36\n",
            "New Test r2 = 0.8218740628640144\n",
            "All features Test r2 = 0.8176628877400767\n",
            "Drop in r2 = -0.00421117512393776\n",
            "remove:  KitchenAbvGr\n",
            "\n",
            "testing feature:  BsmtFullBath  which is feature  3  out of  36\n",
            "New Test r2 = 0.8216199474312881\n",
            "All features Test r2 = 0.8218740628640144\n",
            "Drop in r2 = 0.00025411543272635395\n",
            "remove:  BsmtFullBath\n",
            "\n",
            "testing feature:  BsmtHalfBath  which is feature  4  out of  36\n",
            "New Test r2 = 0.8241857928140686\n",
            "All features Test r2 = 0.8216199474312881\n",
            "Drop in r2 = -0.002565845382780485\n",
            "remove:  BsmtHalfBath\n",
            "\n",
            "testing feature:  GarageCars  which is feature  5  out of  36\n",
            "New Test r2 = 0.8234478263522329\n",
            "All features Test r2 = 0.8241857928140686\n",
            "Drop in r2 = 0.0007379664618356596\n",
            "remove:  GarageCars\n",
            "\n",
            "testing feature:  MiscVal  which is feature  6  out of  36\n",
            "New Test r2 = 0.8245560565485206\n",
            "All features Test r2 = 0.8234478263522329\n",
            "Drop in r2 = -0.0011082301962876961\n",
            "remove:  MiscVal\n",
            "\n",
            "testing feature:  FullBath  which is feature  7  out of  36\n",
            "New Test r2 = 0.8142130912663263\n",
            "All features Test r2 = 0.8245560565485206\n",
            "Drop in r2 =0.01034296528219425\n",
            "keep:  FullBath\n",
            "\n",
            "testing feature:  HalfBath  which is feature  8  out of  36\n",
            "New Test r2 = 0.8223914501432945\n",
            "All features Test r2 = 0.8245560565485206\n",
            "Drop in r2 =0.0021646064052260883\n",
            "keep:  HalfBath\n",
            "\n",
            "testing feature:  Fireplaces  which is feature  9  out of  36\n",
            "New Test r2 = 0.814062191310915\n",
            "All features Test r2 = 0.8245560565485206\n",
            "Drop in r2 =0.010493865237605648\n",
            "keep:  Fireplaces\n",
            "\n",
            "testing feature:  3SsnPorch  which is feature  10  out of  36\n",
            "New Test r2 = 0.8228884035226094\n",
            "All features Test r2 = 0.8245560565485206\n",
            "Drop in r2 =0.0016676530259112088\n",
            "keep:  3SsnPorch\n",
            "\n",
            "testing feature:  EnclosedPorch  which is feature  11  out of  36\n",
            "New Test r2 = 0.8216894685951396\n",
            "All features Test r2 = 0.8245560565485206\n",
            "Drop in r2 =0.002866587953381017\n",
            "keep:  EnclosedPorch\n",
            "\n",
            "testing feature:  TotRmsAbvGrd  which is feature  12  out of  36\n",
            "New Test r2 = 0.8200291572058384\n",
            "All features Test r2 = 0.8245560565485206\n",
            "Drop in r2 =0.004526899342682245\n",
            "keep:  TotRmsAbvGrd\n",
            "\n",
            "testing feature:  BedroomAbvGr  which is feature  13  out of  36\n",
            "New Test r2 = 0.8236742809864477\n",
            "All features Test r2 = 0.8245560565485206\n",
            "Drop in r2 = 0.0008817755620729173\n",
            "remove:  BedroomAbvGr\n",
            "\n",
            "testing feature:  PoolArea  which is feature  14  out of  36\n",
            "New Test r2 = 0.8123855405578879\n",
            "All features Test r2 = 0.8236742809864477\n",
            "Drop in r2 =0.011288740428559763\n",
            "keep:  PoolArea\n",
            "\n",
            "testing feature:  ScreenPorch  which is feature  15  out of  36\n",
            "New Test r2 = 0.8259976977255308\n",
            "All features Test r2 = 0.8236742809864477\n",
            "Drop in r2 = -0.002323416739083095\n",
            "remove:  ScreenPorch\n",
            "\n",
            "testing feature:  YrSold  which is feature  16  out of  36\n",
            "New Test r2 = 0.8238306513646853\n",
            "All features Test r2 = 0.8259976977255308\n",
            "Drop in r2 =0.002167046360845526\n",
            "keep:  YrSold\n",
            "\n",
            "testing feature:  BsmtFinSF2  which is feature  17  out of  36\n",
            "New Test r2 = 0.8170781108452227\n",
            "All features Test r2 = 0.8259976977255308\n",
            "Drop in r2 =0.008919586880308028\n",
            "keep:  BsmtFinSF2\n",
            "\n",
            "testing feature:  2ndFlrSF  which is feature  18  out of  36\n",
            "New Test r2 = 0.816373763701784\n",
            "All features Test r2 = 0.8259976977255308\n",
            "Drop in r2 =0.009623934023746727\n",
            "keep:  2ndFlrSF\n",
            "\n",
            "testing feature:  YearRemodAdd  which is feature  19  out of  36\n",
            "New Test r2 = 0.8258583975235964\n",
            "All features Test r2 = 0.8259976977255308\n",
            "Drop in r2 = 0.0001393002019344225\n",
            "remove:  YearRemodAdd\n",
            "\n",
            "testing feature:  WoodDeckSF  which is feature  20  out of  36\n",
            "New Test r2 = 0.8211710487205289\n",
            "All features Test r2 = 0.8258583975235964\n",
            "Drop in r2 =0.004687348803067426\n",
            "keep:  WoodDeckSF\n",
            "\n",
            "testing feature:  OpenPorchSF  which is feature  21  out of  36\n",
            "New Test r2 = 0.8224357188998381\n",
            "All features Test r2 = 0.8258583975235964\n",
            "Drop in r2 =0.003422678623758224\n",
            "keep:  OpenPorchSF\n",
            "\n",
            "testing feature:  OverallQual  which is feature  22  out of  36\n",
            "New Test r2 = 0.7923627378533993\n",
            "All features Test r2 = 0.8258583975235964\n",
            "Drop in r2 =0.033495659670197075\n",
            "keep:  OverallQual\n",
            "\n",
            "testing feature:  TotalBsmtSF  which is feature  23  out of  36\n",
            "New Test r2 = 0.8159251371651921\n",
            "All features Test r2 = 0.8258583975235964\n",
            "Drop in r2 =0.00993326035840425\n",
            "keep:  TotalBsmtSF\n",
            "\n",
            "testing feature:  MasVnrArea  which is feature  24  out of  36\n",
            "New Test r2 = 0.8531558041299072\n",
            "All features Test r2 = 0.8258583975235964\n",
            "Drop in r2 = -0.02729740660631086\n",
            "remove:  MasVnrArea\n",
            "\n",
            "testing feature:  OverallCond  which is feature  25  out of  36\n",
            "New Test r2 = 0.8446694034811906\n",
            "All features Test r2 = 0.8531558041299072\n",
            "Drop in r2 =0.008486400648716641\n",
            "keep:  OverallCond\n",
            "\n",
            "testing feature:  MSSubClass  which is feature  26  out of  36\n",
            "New Test r2 = 0.851514246867042\n",
            "All features Test r2 = 0.8531558041299072\n",
            "Drop in r2 =0.0016415572628651898\n",
            "keep:  MSSubClass\n",
            "\n",
            "testing feature:  MoSold  which is feature  27  out of  36\n",
            "New Test r2 = 0.863875155480818\n",
            "All features Test r2 = 0.8531558041299072\n",
            "Drop in r2 = -0.010719351350910733\n",
            "remove:  MoSold\n",
            "\n",
            "testing feature:  GarageYrBlt  which is feature  28  out of  36\n",
            "New Test r2 = 0.8607770586428112\n",
            "All features Test r2 = 0.863875155480818\n",
            "Drop in r2 =0.0030980968380067697\n",
            "keep:  GarageYrBlt\n",
            "\n",
            "testing feature:  YearBuilt  which is feature  29  out of  36\n",
            "New Test r2 = 0.8597803158007506\n",
            "All features Test r2 = 0.863875155480818\n",
            "Drop in r2 =0.004094839680067297\n",
            "keep:  YearBuilt\n",
            "\n",
            "testing feature:  1stFlrSF  which is feature  30  out of  36\n",
            "New Test r2 = 0.8571840255319483\n",
            "All features Test r2 = 0.863875155480818\n",
            "Drop in r2 =0.006691129948869667\n",
            "keep:  1stFlrSF\n",
            "\n",
            "testing feature:  BsmtFinSF1  which is feature  31  out of  36\n",
            "New Test r2 = 0.8535897492997575\n",
            "All features Test r2 = 0.863875155480818\n",
            "Drop in r2 =0.010285406181060441\n",
            "keep:  BsmtFinSF1\n",
            "\n",
            "testing feature:  GarageArea  which is feature  32  out of  36\n",
            "New Test r2 = 0.8619347524274193\n",
            "All features Test r2 = 0.863875155480818\n",
            "Drop in r2 =0.0019404030533985983\n",
            "keep:  GarageArea\n",
            "\n",
            "testing feature:  GrLivArea  which is feature  33  out of  36\n",
            "New Test r2 = 0.8576575601588515\n",
            "All features Test r2 = 0.863875155480818\n",
            "Drop in r2 =0.006217595321966418\n",
            "keep:  GrLivArea\n",
            "\n",
            "testing feature:  BsmtUnfSF  which is feature  34  out of  36\n",
            "New Test r2 = 0.8654469148741569\n",
            "All features Test r2 = 0.863875155480818\n",
            "Drop in r2 = -0.0015717593933389784\n",
            "remove:  BsmtUnfSF\n",
            "\n",
            "testing feature:  LotArea  which is feature  35  out of  36\n",
            "New Test r2 = 0.8532509341202452\n",
            "All features Test r2 = 0.8654469148741569\n",
            "Drop in r2 =0.012195980753911706\n",
            "keep:  LotArea\n",
            "\n",
            "testing feature:  LotFrontage  which is feature  36  out of  36\n",
            "New Test r2 = 0.887427764266262\n",
            "All features Test r2 = 0.8654469148741569\n",
            "Drop in r2 = -0.021980849392105095\n",
            "remove:  LotFrontage\n",
            "DONE!!\n",
            "total features to remove:  13\n",
            "total features to keep:  23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0HT9pyAhWyu",
        "colab_type": "code",
        "colab": {},
        "outputId": "3d5fdd48-137c-4de5-8881-f73d11c0963f"
      },
      "source": [
        "# capture the 23 selected features\n",
        "seed_val = 1000000000\n",
        "np.random.seed(seed_val)\n",
        "\n",
        "# build initial model\n",
        "final_xgb = xgb.XGBRegressor(\n",
        "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
        "\n",
        "# fit the model with the selected features\n",
        "final_xgb.fit(X_train[features_to_keep], y_train)\n",
        "\n",
        "# make predictions\n",
        "y_pred_test = final_xgb.predict(X_test[features_to_keep])\n",
        "\n",
        "# calculate roc-auc\n",
        "r2_score_final = r2_score(y_test, y_pred_test)\n",
        "print('Test selected features r2 = %f' % (r2_score_final))\n",
        "print('Test all features r2 = %f' % (r2_score_all))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test selected features r2 = 0.863848\n",
            "Test all features r2 = 0.887428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcJvsWL-hWy2",
        "colab_type": "text"
      },
      "source": [
        "The model built with 23 features seems to be more predictive than the one built with the total number of features (r2 = 0.88 vs 0.81). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_L6TKVEhWy3",
        "colab_type": "text"
      },
      "source": [
        "That is all for this lecture, I hope you enjoyed it and see you in the next one!"
      ]
    }
  ]
}