{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": true
    },
    "colab": {
      "name": "3.6 Linear_Regression_coefficients.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG9ol5XYNGNX",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression Coefficients\n",
        "\n",
        "Linear regression is a straightforward approach for predicting a quantitative response Y on the basis of a different predictor variable X1, X2, ... Xn. It assumes that there is a linear relationship between X(s) and Y. Mathematically, we can write this linear relationship as Y ≈ β0 + β1X1 + β2X2 + ... + βnXn.\n",
        "\n",
        "**The magnitud of the coefficients is directly influenced by the scale of the features**. Therefore, to compare coefficients accross features, it is importance to have all features within the same scale. This is why, normalisation is important for variable importance and feature selection in linear models. Normalisation is important as well for model performance.\n",
        "\n",
        "In addition, Linear Regression makes the following assumptions over the predictor variables X:\n",
        "- Linear relationship with the outcome Y\n",
        "- Multivariate normality (X should follow a Gaussian distribution)\n",
        "- No or little multicollinearity (Xs should not be linearly related to one another)\n",
        "- Homoscedasticity (variance should be the same)\n",
        "\n",
        "Homoscedasticity, also known as homogeneity of variance, describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables (Xs) and the dependent variable (Y)) is the same across all values of the independent variables.\n",
        "\n",
        "Therefore, there are a lot of assumptions that need to be met in order to make a fair comparison of the features by using only their regression coefficients.\n",
        "\n",
        "In addition, these coefficients may be penalised by regularisation, therefore being smaller than if we were to compare only that individual feature with the target.\n",
        "\n",
        "Having said this, you can still select features based on linear regression coefficients, provided you keep all of these in mind at the time of analysing the outcome.\n",
        "\n",
        "Personally, this is not my feature selection method of choice, although I find it useful to interpret the output of the model.\n",
        "\n",
        "\n",
        "I will demonstrate how to select features based on the regression coefficients using sklearn on a regression problem, using the House Price dataset from Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWhXtQfPNGNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPfwzYokNGNi",
        "colab_type": "text"
      },
      "source": [
        "### Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmnJ1wZINGNm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16a4900b-6cb1-43b1-8352-b7bee9ee8669"
      },
      "source": [
        "# load dataset\n",
        "data = pd.read_csv('houseprice_train.csv')\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 81)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97d43ln0NGNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "554a42ea-d386-40a1-c3e4-f09a26c7e5e4"
      },
      "source": [
        "# In practice, feature selection should be done after data pre-processing,\n",
        "# so ideally, all the categorical variables are encoded into numbers,\n",
        "# and then you can assess how deterministic they are of the target\n",
        "\n",
        "# here for simplicity I will use only numerical variables\n",
        "# select numerical columns:\n",
        "\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
        "data = data[numerical_vars]\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 38)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UANIJJ6VNGNz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1b821cf-e2b7-4f39-9416-0ea73d048354"
      },
      "source": [
        "# separate train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop(labels=['SalePrice'], axis=1),\n",
        "    data['SalePrice'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1022, 37), (438, 37))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzCle3bCNGN5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "550ea692-0912-498c-bf59-8754d35d1670"
      },
      "source": [
        "# the features in the house dataset are in very\n",
        "# different scales\n",
        "# in addition, to select features based on coefficients,\n",
        "# the features need to be in the same scale\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train.fillna(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler(copy=True, with_mean=True, with_std=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1TQSRF89NGN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7f1335e9-ca6c-4581-b1d4-aa97ba955887"
      },
      "source": [
        "# here, again I will train a Linear regression and select\n",
        "# features with higher coefficients all in one line of code.\n",
        "\n",
        "# the LinearRegression object from sklearn is a non-regularised\n",
        "# linear method. It fits by matrix multiplication and not \n",
        "# gradient descent.\n",
        "\n",
        "# therefore I don't need to specify penalty and other parameters\n",
        "\n",
        "sel_ = SelectFromModel(LinearRegression())\n",
        "sel_.fit(scaler.transform(X_train.fillna(0)), y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SelectFromModel(estimator=LinearRegression(copy_X=True, fit_intercept=True,\n",
              "                                           n_jobs=None, normalize=False),\n",
              "                max_features=None, norm_order=1, prefit=False, threshold=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RZnAobriNGOI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4755834-b935-4827-8679-3c1e881ebb7a"
      },
      "source": [
        "# let's count the number of variables selected\n",
        "selected_feat = X_train.columns[(sel_.get_support())]\n",
        "len(selected_feat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIQ1eh4ENGON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "5824d21f-6de7-4118-9de1-0d7eabe024bb"
      },
      "source": [
        "# and now let's plot the histogram of absolute coefficients\n",
        "\n",
        "pd.Series(np.abs(sel_.estimator_.coef_).ravel()).hist(bins=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5aed663a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEGCAYAAAB8Ys7jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnCTciLVUAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zq22M9ONGOT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "31825c8e-5f7c-41bd-84c8-8a236b3bdf3c"
      },
      "source": [
        "# and now, let's compare the  amount of selected features\n",
        "# with the amount of features which coefficient is above the\n",
        "# mean coefficient, to make sure we understand the output of\n",
        "# sklearn\n",
        "\n",
        "print('total features: {}'.format((X_train.shape[1])))\n",
        "print('selected features: {}'.format(len(selected_feat)))\n",
        "print('features with coefficients greater than the mean coefficient: {}'.format(\n",
        "    np.sum(np.abs(sel_.estimator_.coef_) > np.abs(sel_.estimator_.coef_).mean())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total features: 37\n",
            "selected features: 7\n",
            "features with coefficients greater than the mean coefficient: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYER486GNGOg",
        "colab_type": "text"
      },
      "source": [
        "Same as before, it selects those features which coefficient are greater than the mean of all feature coefficients.\n",
        "\n",
        "That is all for this lecture, I hope you enjoyed it and see you in the next one!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH99ChPaNGOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}