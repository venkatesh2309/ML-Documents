{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": true
    },
    "colab": {
      "name": "3.8 Random_forest_importance.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws8BZEmEPDrY",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest importance\n",
        "\n",
        "Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.\n",
        "\n",
        "Random forests consist of 4-12 hundred decision trees, each of them built over a random extraction of the observations from the dataset and a random extraction of the features. Not every tree sees all the features or all the observations, and this guarantees that the trees are de-correlated and therefore less prone to over-fitting. Each tree is also a sequence of yes-no questions based on a single or combination of features. At each node (this is at each question), the three divides the dataset into 2 buckets, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the importance of each feature is derived by how \"pure\" each of the buckets is. \n",
        "\n",
        "For classification, the measure of impurity is either the Gini impurity or the information gain/entropy. For regression the  measure of impurity is variance. Therefore, when training a tree, it is possible to compute how much each feature decreases the impurity. The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease from each feature can be averaged across trees to determine the final importance of the variable.\n",
        "\n",
        "To give you a better intuition, features that are selected at the top of the trees are in general more important than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains.\n",
        "\n",
        "**Note**\n",
        "- Random Forests and decision trees in general give preference to features with high cardinality\n",
        "- Correlated features will be given equal or similar importance, but overall reduced importance compared to the same tree built without correlated counterparts.\n",
        "\n",
        "I will demonstrate how to select features based on tree importance using sklearn on a regression and classification problem. For classification I will use the Paribas claims dataset from Kaggle. For regression, the House Price dataset from Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoUR2l4iPQ5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "cc335577-303d-4d0f-8b86-c45f3932f469"
      },
      "source": [
        "!pip install --user kaggle\n",
        "!mkdir .kaggle\n",
        "import json\n",
        "token = {\"username\":\"#####\",\"key\":\"#####\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "mkdir: cannot create directory ‘.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG5VC7RKbyd1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e90c2a24-bf39-4f12-fca9-b681132b60a0"
      },
      "source": [
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c santander-customer-satisfaction\n",
        "!unzip train.csv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv.zip to /content\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 130MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/612k [00:00<?, ?B/s]\n",
            "100% 612k/612k [00:00<00:00, 183MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "  0% 0.00/4.05M [00:00<?, ?B/s]\n",
            "100% 4.05M/4.05M [00:00<00:00, 66.2MB/s]\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBmGigXaPDrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.feature_selection import SelectFromModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G5DF-UfPDrj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cb0389c-2d99-4c89-cd4d-9008d3d75c25"
      },
      "source": [
        "# load dataset\n",
        "data = pd.read_csv('train.csv', nrows=50000)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 371)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrK00T3TPDrp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "6ea481b4-2cd6-40b2-96a8-c65d48f9a416"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>var3</th>\n",
              "      <th>var15</th>\n",
              "      <th>imp_ent_var16_ult1</th>\n",
              "      <th>imp_op_var39_comer_ult1</th>\n",
              "      <th>imp_op_var39_comer_ult3</th>\n",
              "      <th>imp_op_var40_comer_ult1</th>\n",
              "      <th>imp_op_var40_comer_ult3</th>\n",
              "      <th>imp_op_var40_efect_ult1</th>\n",
              "      <th>imp_op_var40_efect_ult3</th>\n",
              "      <th>imp_op_var40_ult1</th>\n",
              "      <th>imp_op_var41_comer_ult1</th>\n",
              "      <th>imp_op_var41_comer_ult3</th>\n",
              "      <th>imp_op_var41_efect_ult1</th>\n",
              "      <th>imp_op_var41_efect_ult3</th>\n",
              "      <th>imp_op_var41_ult1</th>\n",
              "      <th>imp_op_var39_efect_ult1</th>\n",
              "      <th>imp_op_var39_efect_ult3</th>\n",
              "      <th>imp_op_var39_ult1</th>\n",
              "      <th>imp_sal_var16_ult1</th>\n",
              "      <th>ind_var1_0</th>\n",
              "      <th>ind_var1</th>\n",
              "      <th>ind_var2_0</th>\n",
              "      <th>ind_var2</th>\n",
              "      <th>ind_var5_0</th>\n",
              "      <th>ind_var5</th>\n",
              "      <th>ind_var6_0</th>\n",
              "      <th>ind_var6</th>\n",
              "      <th>ind_var8_0</th>\n",
              "      <th>ind_var8</th>\n",
              "      <th>ind_var12_0</th>\n",
              "      <th>ind_var12</th>\n",
              "      <th>ind_var13_0</th>\n",
              "      <th>ind_var13_corto_0</th>\n",
              "      <th>ind_var13_corto</th>\n",
              "      <th>ind_var13_largo_0</th>\n",
              "      <th>ind_var13_largo</th>\n",
              "      <th>ind_var13_medio_0</th>\n",
              "      <th>ind_var13_medio</th>\n",
              "      <th>ind_var13</th>\n",
              "      <th>...</th>\n",
              "      <th>saldo_medio_var5_ult1</th>\n",
              "      <th>saldo_medio_var5_ult3</th>\n",
              "      <th>saldo_medio_var8_hace2</th>\n",
              "      <th>saldo_medio_var8_hace3</th>\n",
              "      <th>saldo_medio_var8_ult1</th>\n",
              "      <th>saldo_medio_var8_ult3</th>\n",
              "      <th>saldo_medio_var12_hace2</th>\n",
              "      <th>saldo_medio_var12_hace3</th>\n",
              "      <th>saldo_medio_var12_ult1</th>\n",
              "      <th>saldo_medio_var12_ult3</th>\n",
              "      <th>saldo_medio_var13_corto_hace2</th>\n",
              "      <th>saldo_medio_var13_corto_hace3</th>\n",
              "      <th>saldo_medio_var13_corto_ult1</th>\n",
              "      <th>saldo_medio_var13_corto_ult3</th>\n",
              "      <th>saldo_medio_var13_largo_hace2</th>\n",
              "      <th>saldo_medio_var13_largo_hace3</th>\n",
              "      <th>saldo_medio_var13_largo_ult1</th>\n",
              "      <th>saldo_medio_var13_largo_ult3</th>\n",
              "      <th>saldo_medio_var13_medio_hace2</th>\n",
              "      <th>saldo_medio_var13_medio_hace3</th>\n",
              "      <th>saldo_medio_var13_medio_ult1</th>\n",
              "      <th>saldo_medio_var13_medio_ult3</th>\n",
              "      <th>saldo_medio_var17_hace2</th>\n",
              "      <th>saldo_medio_var17_hace3</th>\n",
              "      <th>saldo_medio_var17_ult1</th>\n",
              "      <th>saldo_medio_var17_ult3</th>\n",
              "      <th>saldo_medio_var29_hace2</th>\n",
              "      <th>saldo_medio_var29_hace3</th>\n",
              "      <th>saldo_medio_var29_ult1</th>\n",
              "      <th>saldo_medio_var29_ult3</th>\n",
              "      <th>saldo_medio_var33_hace2</th>\n",
              "      <th>saldo_medio_var33_hace3</th>\n",
              "      <th>saldo_medio_var33_ult1</th>\n",
              "      <th>saldo_medio_var33_ult3</th>\n",
              "      <th>saldo_medio_var44_hace2</th>\n",
              "      <th>saldo_medio_var44_hace3</th>\n",
              "      <th>saldo_medio_var44_ult1</th>\n",
              "      <th>saldo_medio_var44_ult3</th>\n",
              "      <th>var38</th>\n",
              "      <th>TARGET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39205.170000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>300.0</td>\n",
              "      <td>122.22</td>\n",
              "      <td>300.0</td>\n",
              "      <td>240.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49278.030000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67333.770000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>91.56</td>\n",
              "      <td>138.84</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64007.970000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>39</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>40501.08</td>\n",
              "      <td>13501.47</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>85501.89</td>\n",
              "      <td>85501.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117310.979016</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 371 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  var3  var15  ...  saldo_medio_var44_ult3          var38  TARGET\n",
              "0   1     2     23  ...                     0.0   39205.170000       0\n",
              "1   3     2     34  ...                     0.0   49278.030000       0\n",
              "2   4     2     23  ...                     0.0   67333.770000       0\n",
              "3   8     2     37  ...                     0.0   64007.970000       0\n",
              "4  10     2     39  ...                     0.0  117310.979016       0\n",
              "\n",
              "[5 rows x 371 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCccReyWPDrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44b7d1c3-9b1a-49c3-d884-f8dfbb5e3ed2"
      },
      "source": [
        "# In practice, feature selection should be done after data pre-processing,\n",
        "# so ideally, all the categorical variables are encoded into numbers,\n",
        "# and then you can assess how deterministic they are of the target\n",
        "\n",
        "# here for simplicity I will use only numerical variables\n",
        "# select numerical columns:\n",
        "\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
        "data = data[numerical_vars]\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 371)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R92XuRrPDr2",
        "colab_type": "text"
      },
      "source": [
        "### Important\n",
        "\n",
        "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luqhw12tPDr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "791b78d5-9189-43f3-9a81-51903fccb5c5"
      },
      "source": [
        "# separate train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop(labels=['TARGET', 'ID'], axis=1),\n",
        "    data['TARGET'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((35000, 369), (15000, 369))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "45cMA8ioPDr9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "4b0ed187-ec3f-4fb1-9084-dc0f5047ba21"
      },
      "source": [
        "# here I will do the model fitting and feature selection\n",
        "# altogether in one line of code\n",
        "\n",
        "# first I specify the Random Forest instance, indicating\n",
        "# the number of trees\n",
        "\n",
        "# Then I use the selectFromModel object from sklearn\n",
        "# to automatically select the features\n",
        "\n",
        "# SelectFrom model will select those features which importance\n",
        "# is greater than the mean importance of all the features\n",
        "# by default, but you can alter this threshold if you want to\n",
        "\n",
        "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
        "sel_.fit(X_train.fillna(0), y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                                 class_weight=None,\n",
              "                                                 criterion='gini',\n",
              "                                                 max_depth=None,\n",
              "                                                 max_features='auto',\n",
              "                                                 max_leaf_nodes=None,\n",
              "                                                 max_samples=None,\n",
              "                                                 min_impurity_decrease=0.0,\n",
              "                                                 min_impurity_split=None,\n",
              "                                                 min_samples_leaf=1,\n",
              "                                                 min_samples_split=2,\n",
              "                                                 min_weight_fraction_leaf=0.0,\n",
              "                                                 n_estimators=100, n_jobs=None,\n",
              "                                                 oob_score=False,\n",
              "                                                 random_state=None, verbose=0,\n",
              "                                                 warm_start=False),\n",
              "                max_features=None, norm_order=1, prefit=False, threshold=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km8CbDwVPDsD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "54b2396f-5a09-41fe-ca5c-4ee6d570bc46"
      },
      "source": [
        "# this command let's me visualise those features that were selected.\n",
        "\n",
        "# sklearn will select those features which importance values\n",
        "# are greater than the mean of all the coefficients.\n",
        "\n",
        "sel_.get_support()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True, False, False, False, False,\n",
              "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "        True, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False,  True, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False,  True, False, False,\n",
              "        True, False, False, False,  True, False, False, False, False,\n",
              "       False, False, False,  True, False, False, False, False, False,\n",
              "       False, False, False,  True, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False,  True, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False,  True, False, False, False, False,  True, False,\n",
              "       False,  True, False, False,  True, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False,  True, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False,  True,  True,  True,  True,  True,\n",
              "        True,  True, False, False, False, False, False, False, False,\n",
              "       False,  True, False, False,  True, False, False, False, False,\n",
              "       False,  True, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False,  True,\n",
              "        True,  True,  True, False,  True,  True,  True,  True, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cGFS22ZpPDsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81cff229-598a-4237-f28a-99d3a6571a2f"
      },
      "source": [
        "# let's make a list and count the selected features\n",
        "selected_feat = X_train.columns[(sel_.get_support())]\n",
        "len(selected_feat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei3_KGiQPDsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "57104384-e64e-4429-93a4-887569669230"
      },
      "source": [
        "selected_feat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['var3', 'var15', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1',\n",
              "       'imp_op_var39_comer_ult3', 'imp_op_var41_comer_ult1',\n",
              "       'imp_op_var41_comer_ult3', 'imp_op_var41_efect_ult1',\n",
              "       'imp_op_var41_efect_ult3', 'imp_op_var41_ult1',\n",
              "       'imp_op_var39_efect_ult1', 'imp_op_var39_efect_ult3',\n",
              "       'imp_op_var39_ult1', 'ind_var30', 'num_var4', 'num_op_var41_hace2',\n",
              "       'num_op_var41_ult3', 'num_op_var39_ult3', 'num_var30', 'num_var35',\n",
              "       'saldo_var5', 'saldo_var30', 'saldo_var37', 'saldo_var42', 'var36',\n",
              "       'imp_trans_var37_ult1', 'num_var22_hace2', 'num_var22_hace3',\n",
              "       'num_var22_ult1', 'num_var22_ult3', 'num_med_var22_ult3',\n",
              "       'num_med_var45_ult3', 'num_meses_var5_ult3', 'num_meses_var39_vig_ult3',\n",
              "       'num_op_var39_comer_ult3', 'num_op_var41_comer_ult3', 'num_var45_hace2',\n",
              "       'num_var45_hace3', 'num_var45_ult1', 'num_var45_ult3',\n",
              "       'saldo_medio_var5_hace2', 'saldo_medio_var5_hace3',\n",
              "       'saldo_medio_var5_ult1', 'saldo_medio_var5_ult3', 'var38'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ytau4zJPDsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "2aedf8f9-6d2e-4b6d-ac04-9ea5a40696a0"
      },
      "source": [
        "# and now let's plot the distribution of importances\n",
        "\n",
        "pd.Series(sel_.estimator_.feature_importances_.ravel()).hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcd0bf44198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9P8BmS9a/6kpgrUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETuVgEx8PDse",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4f2f59dc-a393-42e6-96b1-14e1201e0b26"
      },
      "source": [
        "# and now, let's compare the  amount of selected features\n",
        "# with the amount of features which importance is above the\n",
        "# mean importance, to make sure we understand the output of\n",
        "# sklearn\n",
        "\n",
        "print('total features: {}'.format((X_train.shape[1])))\n",
        "print('selected features: {}'.format(len(selected_feat)))\n",
        "print('features with coefficients greater than the mean coefficient: {}'.format(\n",
        "    np.sum(sel_.estimator_.feature_importances_ > sel_.estimator_.feature_importances_.mean())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total features: 369\n",
            "selected features: 45\n",
            "features with coefficients greater than the mean coefficient: 45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQh0_UogPDsj",
        "colab_type": "text"
      },
      "source": [
        "You can of course tune the parameters of the Decision Tree.\n",
        "\n",
        "Where we put the cut-off to select features is a bit arbitrary. One way is to select the top 10, 20 features. Alternatively, the top 10th percentile. \n",
        "For this, you can use mutual info in combination with SelectKBest or SelectPercentile from sklearn. See below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z8XzP-zPDsk",
        "colab_type": "text"
      },
      "source": [
        "### Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXEuRBQsPDsm",
        "colab_type": "code",
        "colab": {},
        "outputId": "8dec67e9-7462-4552-ffba-aea2221de206"
      },
      "source": [
        "# load dataset\n",
        "data = pd.read_csv('houseprice.csv', nrows=50000)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 81)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVPfFP4HPDsr",
        "colab_type": "code",
        "colab": {},
        "outputId": "4470929f-4fab-47f4-a0d9-14dfd739f8c5"
      },
      "source": [
        "# In practice, feature selection should be done after data pre-processing,\n",
        "# so ideally, all the categorical variables are encoded into numbers,\n",
        "# and then you can assess how deterministic they are of the target\n",
        "\n",
        "# here for simplicity I will use only numerical variables\n",
        "# select numerical columns:\n",
        "\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
        "data = data[numerical_vars]\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 38)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUBiKYROPDsv",
        "colab_type": "code",
        "colab": {},
        "outputId": "58b424c2-6526-48f6-b78a-4127bf910e31"
      },
      "source": [
        "# separate train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop(labels=['SalePrice'], axis=1),\n",
        "    data['SalePrice'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1022, 37), (438, 37))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6UiZlJZlPDs4",
        "colab_type": "code",
        "colab": {},
        "outputId": "2472d413-9e7a-448f-d775-2aab19b96753"
      },
      "source": [
        "# here I will do the model fitting and feature selection\n",
        "# altogether in one line of code\n",
        "\n",
        "# first I specify the Random Forest instance, indicating\n",
        "# the number of trees (the default value in sklearn is 10\n",
        "\n",
        "# Then I use the selectFromModel object from sklearn\n",
        "# to automatically select the features\n",
        "\n",
        "# SelectFrom model will select those features which importance\n",
        "# is greater than the mean importance of all the features\n",
        "# by default, but you can alter this threshold if you want to\n",
        "\n",
        "sel_ = SelectFromModel(RandomForestRegressor(n_estimators=100))\n",
        "sel_.fit(X_train.fillna(0), y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SelectFromModel(estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
              "           max_features='auto', max_leaf_nodes=None,\n",
              "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
              "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "           n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
              "           verbose=0, warm_start=False),\n",
              "        prefit=False, threshold=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfqN0842PDs8",
        "colab_type": "code",
        "colab": {},
        "outputId": "8081635a-6534-4846-9422-cac74b3ee2fb"
      },
      "source": [
        "# let's make a list and count the selected features\n",
        "selected_feat = X_train.columns[(sel_.get_support())]\n",
        "len(selected_feat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "scVGtjuaPDtD",
        "colab_type": "code",
        "colab": {},
        "outputId": "36ca5959-8d52-484d-95bc-15c2e865e58a"
      },
      "source": [
        "# and now, let's compare the  amount of selected features\n",
        "# with the amount of features which importance is above the\n",
        "# mean importance, to make sure we understand the output of\n",
        "# sklearn\n",
        "\n",
        "print('total features: {}'.format((X_train.shape[1])))\n",
        "print('selected features: {}'.format(len(selected_feat)))\n",
        "print('features with coefficients greater than the mean coefficient: {}'.format(\n",
        "    np.sum(sel_.estimator_.feature_importances_ > sel_.estimator_.feature_importances_.mean())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total features: 37\n",
            "selected features: 5\n",
            "features with coefficients greater than the mean coefficient: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB2um2fsPDtI",
        "colab_type": "text"
      },
      "source": [
        "Selecting features by using tree derived feature importance is a very srtaightforward, fast and generally accurate way of selecting good features for machine learning. In particular, if you are going to build tree methods.\n",
        "\n",
        "However, as I said, correlated features will show in a tree similar and lowered importance, compared to what their importance would be if the tree was built without correlated counterparts.\n",
        "\n",
        "In situations like this, it is better to select features recursively, rather than altogether like I am doing in this lecture.\n",
        "\n",
        "\n",
        "That is all for this lecture, I hope you enjoyed it and see you in the next one!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giTuzsNqPDtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}